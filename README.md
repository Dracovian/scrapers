# Scrapers
This is going to be a repository for random scrapers I've come to develop over the years. These scripts have been published in the intention of data preservation.

## Description
All scrapers in this repository with a brief description

* calibre-server - Grabs ebooks from a calibre-server
* dafont - Grabs fonts uploaded to dafont.com
* waset - Grabs papers published to waset.org
* xkcd - Grabs comics posted to xkcd.com


## Others
Scrapers in other repositories

* [theoatmeal.com-downloader](https://github.com/LameLemon/theoatmeal.com-downloader) - Grabs all comics from [The Oatmeal](https://theoatmeal.com)

* [zenpencils-downloader](https://github.com/LameLemon/zenpencils-downloader) - Grabs all comics from [zen pencils](https://zenpencils.com/)

* [RedditImageBackup](https://github.com/LameLemon/RedditImageBackup) - Grabs all images, gifs, videos and text posts from Reddit 

* [archivers by nektro](https://github.com/nektro/archivers) - A collection of scripts to mass download data from various sites

* [Bios-Archival-Standards by BiosPlus](https://github.com/BiosPlus/Bios-Archival-Standards) - A collection of gists and notes to help me [BiosPlus] standardize my archival efforts accross the board

* [lazynlp by chiphuyen](https://github.com/chiphuyen/lazynlp) - Library to scrape and clean web pages to create massive datasets

* [wayback-machine-downloader](https://github.com/hartator/wayback-machine-downloader) - Download an entire website from the Wayback Machine

* [PixivDownloader](https://github.com/nonPointer/PixivDownloader) - Simple batch tool to download one's image from Pixiv

* [archivebot-archives](https://github.com/nonPointer/PixivDownloader) - This repository containes a list of files in the ArchiveBot Collection on the Internet Archive and the corresponding codes.

* [ArchiveBot](https://github.com/ArchiveTeam/ArchiveBot) - ArchiveBot, an IRC bot for archiving websites
